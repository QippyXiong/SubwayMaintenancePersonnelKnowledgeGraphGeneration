{
    "train_params": { // 训练用参数
        "epochs"            : 1   ,
        "batch_size"        : 12  ,
        "warm_up_proportion": 0.01,
        "bert_lr"           : 3e-5,
        "bert_decay"        : 0.01,
        "crf_lr"            : 3e-3,
        "crf_decay"         : 0.0,
        "other_lr"          : 3e-5,
        "other_decay"       : 0.01,
        "eps"               : 1e-5,
        "drop_out"          : 0.1
    },
    "hyper_params": { // 模型参数
        "bert"              : "chinese-bert-wwm-ext", // bert层选用的预训练模型，从model目录下挑一个
        "num_labels"        : 19, // 一般由数据集决定，考虑到模型效果也受此影响，还是作为参数记录
        "seq_len"           : 200,
        "lstm_hidden_size"  : 128,
        "lstm_hidden_layers": 1
    },
    "name": "bert-bilstm-crf_v0.1",
    "dataset": "chinese-literatual-kg" // 目前不能起到任何作用，主要是标注一下模型在哪上面训练的，免得搞错数据集
}
/*
此参数训练结果，之前的代码有问题，增大seq_len，添加到2层LSTM后应该可以取到比下面更好的效果，
但是我没条件跑了，我显存就4GB大
              precision    recall  f1-score   support

    Abstract       0.72      0.41      0.52       135
    Location       0.65      0.62      0.63      1800
      Metric       0.42      0.28      0.34       300
Organization       0.59      0.42      0.49       242
      Person       0.86      0.87      0.87      3981
    Physical       0.00      0.00      0.00         1
       Thing       0.69      0.73      0.71      3111
        Time       0.67      0.54      0.60       895

   micro avg       0.74      0.72      0.73     10465
   macro avg       0.58      0.48      0.52     10465
weighted avg       0.74      0.72      0.73     10465

*/